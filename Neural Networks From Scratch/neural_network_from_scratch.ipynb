{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch - Research Scientist Interview Prep\n",
    "\n",
    "**Author:** Rekhi  \n",
    "**Purpose:** Demonstrate deep understanding of neural network fundamentals  \n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Covers\n",
    "\n",
    "1. **Foundations**: Activation functions and their derivatives\n",
    "2. **Architecture**: Weight initialization strategies\n",
    "3. **Forward Propagation**: How data flows through the network\n",
    "4. **Backpropagation**: Computing gradients via chain rule\n",
    "5. **Gradient Checking**: Verifying backprop correctness\n",
    "6. **Training Loop**: Gradient descent optimization\n",
    "7. **Experiments**: Testing on different data distributions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Plot settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Activation Functions\n",
    "\n",
    "### Why Activation Functions Matter\n",
    "\n",
    "Without activation functions, a neural network is just a linear transformation:  \n",
    "`y = W2(W1*x + b1) + b2 = W_combined*x + b_combined`\n",
    "\n",
    "Activation functions introduce **non-linearity**, allowing networks to learn complex patterns.\n",
    "\n",
    "---\n",
    "\n",
    "### ReLU (Rectified Linear Unit)\n",
    "\n",
    "**Formula:** `f(x) = max(0, x)`  \n",
    "**Derivative:** `f'(x) = 1 if x > 0 else 0`\n",
    "\n",
    "**Why ReLU:**\n",
    "- Computationally efficient (just a threshold)\n",
    "- Reduces vanishing gradient problem (gradient is 1 for positive values)\n",
    "- Sparse activation (neurons can be \"off\")\n",
    "\n",
    "**Limitation:** \"Dying ReLU\" - neurons can get stuck at 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"\n",
    "    ReLU activation: f(z) = max(0, z)\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, activated values\n",
    "    \"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: f'(z) = 1 if z > 0 else 0\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, gradients\n",
    "    \"\"\"\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "**Formula:** `f(x) = 1 / (1 + e^(-x))`  \n",
    "**Derivative:** `f'(x) = f(x) * (1 - f(x))`\n",
    "\n",
    "**Why Sigmoid:**\n",
    "- Outputs in range (0, 1) - good for probabilities\n",
    "- Smooth gradient\n",
    "\n",
    "**Limitations:**\n",
    "- Vanishing gradient (gradient near 0 for large |x|)\n",
    "- Outputs not zero-centered\n",
    "- Computationally expensive (exponential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid activation: f(z) = 1 / (1 + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, activated values in (0, 1)\n",
    "    \"\"\"\n",
    "    # Clip to prevent overflow in exp\n",
    "    z_clipped = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z_clipped))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of sigmoid: f'(z) = f(z) * (1 - f(z))\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, gradients\n",
    "    \"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Formula:** `f(x) = (e^x - e^(-x)) / (e^x + e^(-x))`  \n",
    "**Derivative:** `f'(x) = 1 - f(x)^2`\n",
    "\n",
    "**Why Tanh:**\n",
    "- Zero-centered outputs (-1, 1)\n",
    "- Stronger gradients than sigmoid\n",
    "\n",
    "**Limitation:** Still has vanishing gradient for large |x|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \"\"\"\n",
    "    Tanh activation: f(z) = (e^z - e^(-z)) / (e^z + e^(-z))\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, activated values in (-1, 1)\n",
    "    \"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"\n",
    "    Derivative of tanh: f'(z) = 1 - f(z)^2\n",
    "    \n",
    "    Args:\n",
    "        z: numpy array, pre-activation values\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, gradients\n",
    "    \"\"\"\n",
    "    t = tanh(z)\n",
    "    return 1 - t ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_values = np.linspace(-5, 5, 200)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "\n",
    "# Row 1: Activation functions\n",
    "axes[0, 0].plot(z_values, relu(z_values), 'b-', linewidth=2)\n",
    "axes[0, 0].set_title('ReLU')\n",
    "axes[0, 0].set_xlabel('z')\n",
    "axes[0, 0].set_ylabel('f(z)')\n",
    "axes[0, 0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(z_values, sigmoid(z_values), 'g-', linewidth=2)\n",
    "axes[0, 1].set_title('Sigmoid')\n",
    "axes[0, 1].set_xlabel('z')\n",
    "axes[0, 1].set_ylabel('f(z)')\n",
    "axes[0, 1].axhline(y=0.5, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "axes[0, 2].plot(z_values, tanh(z_values), 'r-', linewidth=2)\n",
    "axes[0, 2].set_title('Tanh')\n",
    "axes[0, 2].set_xlabel('z')\n",
    "axes[0, 2].set_ylabel('f(z)')\n",
    "axes[0, 2].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 2].axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "# Row 2: Derivatives\n",
    "axes[1, 0].plot(z_values, relu_derivative(z_values), 'b-', linewidth=2)\n",
    "axes[1, 0].set_title('ReLU Derivative')\n",
    "axes[1, 0].set_xlabel('z')\n",
    "axes[1, 0].set_ylabel(\"f'(z)\")\n",
    "\n",
    "axes[1, 1].plot(z_values, sigmoid_derivative(z_values), 'g-', linewidth=2)\n",
    "axes[1, 1].set_title('Sigmoid Derivative')\n",
    "axes[1, 1].set_xlabel('z')\n",
    "axes[1, 1].set_ylabel(\"f'(z)\")\n",
    "\n",
    "axes[1, 2].plot(z_values, tanh_derivative(z_values), 'r-', linewidth=2)\n",
    "axes[1, 2].set_title('Tanh Derivative')\n",
    "axes[1, 2].set_xlabel('z')\n",
    "axes[1, 2].set_ylabel(\"f'(z)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- ReLU derivative is constant (1) for positive values - no vanishing gradient\")\n",
    "print(\"- Sigmoid derivative max is 0.25 at z=0 - gradients shrink quickly\")\n",
    "print(\"- Tanh derivative max is 1.0 at z=0 - better than sigmoid but still vanishes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Loss Functions\n",
    "\n",
    "### Mean Squared Error (MSE) - For Regression\n",
    "\n",
    "**Formula:** `L = (1/n) * sum((y_true - y_pred)^2)`  \n",
    "**Derivative w.r.t y_pred:** `dL/dy_pred = (2/n) * (y_pred - y_true)`\n",
    "\n",
    "**Why MSE:**\n",
    "- Penalizes large errors more (squared term)\n",
    "- Differentiable everywhere\n",
    "- Convex for linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss.\n",
    "    \n",
    "    Args:\n",
    "        y_true: numpy array, shape (n_samples, n_outputs), ground truth\n",
    "        y_pred: numpy array, shape (n_samples, n_outputs), predictions\n",
    "    \n",
    "    Returns:\n",
    "        float, mean squared error\n",
    "    \"\"\"\n",
    "    n_samples = y_true.shape[0]\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "\n",
    "def mean_squared_error_derivative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Derivative of MSE w.r.t predictions.\n",
    "    \n",
    "    Args:\n",
    "        y_true: numpy array, shape (n_samples, n_outputs)\n",
    "        y_pred: numpy array, shape (n_samples, n_outputs)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array, gradient of loss w.r.t y_pred\n",
    "    \"\"\"\n",
    "    n_samples = y_true.shape[0]\n",
    "    return (2.0 / n_samples) * (y_pred - y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Weight Initialization\n",
    "\n",
    "### Why Initialization Matters\n",
    "\n",
    "Bad initialization leads to:\n",
    "- **Vanishing gradients**: Weights too small, signals shrink layer by layer\n",
    "- **Exploding gradients**: Weights too large, signals blow up\n",
    "- **Symmetry breaking**: All neurons learn the same thing\n",
    "\n",
    "---\n",
    "\n",
    "### He Initialization (for ReLU)\n",
    "\n",
    "**Formula:** `W ~ N(0, sqrt(2/n_in))`\n",
    "\n",
    "**Why:** ReLU kills half the values (negative), so we need larger variance to compensate.\n",
    "\n",
    "---\n",
    "\n",
    "### Xavier/Glorot Initialization (for Sigmoid/Tanh)\n",
    "\n",
    "**Formula:** `W ~ N(0, sqrt(2/(n_in + n_out)))`\n",
    "\n",
    "**Why:** Keeps variance stable across layers for symmetric activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dimensions, initialization='he', seed=42):\n",
    "    \"\"\"\n",
    "    Initialize weights and biases for all layers.\n",
    "    \n",
    "    Args:\n",
    "        layer_dimensions: list, [n_input, n_hidden1, n_hidden2, ..., n_output]\n",
    "        initialization: str, 'he' for ReLU, 'xavier' for sigmoid/tanh\n",
    "        seed: int, random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        dict, containing W1, b1, W2, b2, ...\n",
    "    \n",
    "    Example:\n",
    "        params = initialize_parameters([2, 4, 4, 1])\n",
    "        # Creates: W1(2,4), b1(1,4), W2(4,4), b2(1,4), W3(4,1), b3(1,1)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    parameters = {}\n",
    "    num_layers = len(layer_dimensions)\n",
    "    \n",
    "    for layer_idx in range(1, num_layers):\n",
    "        n_in = layer_dimensions[layer_idx - 1]   # neurons in previous layer\n",
    "        n_out = layer_dimensions[layer_idx]      # neurons in current layer\n",
    "        \n",
    "        # Choose initialization based on activation\n",
    "        if initialization == 'he':\n",
    "            # He initialization: sqrt(2/n_in)\n",
    "            std = np.sqrt(2.0 / n_in)\n",
    "        elif initialization == 'xavier':\n",
    "            # Xavier initialization: sqrt(2/(n_in + n_out))\n",
    "            std = np.sqrt(2.0 / (n_in + n_out))\n",
    "        else:\n",
    "            # Simple small random\n",
    "            std = 0.01\n",
    "        \n",
    "        # Initialize weights with appropriate scale\n",
    "        parameters[f'W{layer_idx}'] = np.random.randn(n_in, n_out) * std\n",
    "        \n",
    "        # Initialize biases to zero\n",
    "        parameters[f'b{layer_idx}'] = np.zeros((1, n_out))\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "# Test initialization\n",
    "test_params = initialize_parameters([1, 10, 10, 1], initialization='he')\n",
    "print(\"Network Architecture: [1, 10, 10, 1]\")\n",
    "print(\"\\nParameter shapes:\")\n",
    "for key, value in test_params.items():\n",
    "    print(f\"  {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 5: Forward Propagation\n",
    "\n",
    "### The Math\n",
    "\n",
    "For each layer `l`:\n",
    "\n",
    "1. **Linear transformation:** `Z[l] = A[l-1] @ W[l] + b[l]`\n",
    "2. **Activation:** `A[l] = activation(Z[l])`\n",
    "\n",
    "Where:\n",
    "- `A[0] = X` (input)\n",
    "- `@` is matrix multiplication\n",
    "- Last layer typically has no activation (for regression) or softmax (for classification)\n",
    "\n",
    "### Why Cache?\n",
    "\n",
    "We store `Z` and `A` values because backpropagation needs them to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation='relu'):\n",
    "    \"\"\"\n",
    "    Perform forward pass through the network.\n",
    "    \n",
    "    Args:\n",
    "        X: numpy array, shape (n_samples, n_features), input data\n",
    "        parameters: dict, containing W1, b1, W2, b2, ...\n",
    "        activation: str, 'relu', 'sigmoid', or 'tanh'\n",
    "    \n",
    "    Returns:\n",
    "        y_pred: numpy array, shape (n_samples, n_outputs), predictions\n",
    "        cache: dict, containing Z and A for each layer (needed for backprop)\n",
    "    \"\"\"\n",
    "    cache = {'A0': X}  # Store input as A0\n",
    "    A_current = X\n",
    "    num_layers = len(parameters) // 2  # Each layer has W and b\n",
    "    \n",
    "    # Forward through hidden layers (with activation)\n",
    "    for layer_idx in range(1, num_layers):\n",
    "        # Get weights and biases for this layer\n",
    "        W = parameters[f'W{layer_idx}']\n",
    "        b = parameters[f'b{layer_idx}']\n",
    "        \n",
    "        # Linear transformation: Z = A_prev @ W + b\n",
    "        Z = A_current @ W + b\n",
    "        \n",
    "        # Apply activation function\n",
    "        if activation == 'relu':\n",
    "            A_current = relu(Z)\n",
    "        elif activation == 'sigmoid':\n",
    "            A_current = sigmoid(Z)\n",
    "        elif activation == 'tanh':\n",
    "            A_current = tanh(Z)\n",
    "        \n",
    "        # Cache for backpropagation\n",
    "        cache[f'Z{layer_idx}'] = Z\n",
    "        cache[f'A{layer_idx}'] = A_current\n",
    "    \n",
    "    # Output layer (linear - no activation for regression)\n",
    "    W_out = parameters[f'W{num_layers}']\n",
    "    b_out = parameters[f'b{num_layers}']\n",
    "    Z_out = A_current @ W_out + b_out\n",
    "    y_pred = Z_out  # Linear output for regression\n",
    "    \n",
    "    cache[f'Z{num_layers}'] = Z_out\n",
    "    cache[f'A{num_layers}'] = y_pred\n",
    "    \n",
    "    return y_pred, cache\n",
    "\n",
    "\n",
    "# Test forward propagation\n",
    "X_test = np.array([[0.5], [1.0], [1.5]])\n",
    "y_pred_test, cache_test = forward_propagation(X_test, test_params)\n",
    "print(\"Forward propagation test:\")\n",
    "print(f\"  Input shape: {X_test.shape}\")\n",
    "print(f\"  Output shape: {y_pred_test.shape}\")\n",
    "print(f\"  Cache keys: {list(cache_test.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 6: Backpropagation\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "To update `W[l]`, we need `dL/dW[l]`.\n",
    "\n",
    "By chain rule:\n",
    "```\n",
    "dL/dW[l] = dL/dZ[l] * dZ[l]/dW[l]\n",
    "         = dZ[l] * A[l-1].T\n",
    "```\n",
    "\n",
    "And for the next layer back:\n",
    "```\n",
    "dL/dA[l-1] = W[l].T @ dZ[l]\n",
    "dL/dZ[l-1] = dL/dA[l-1] * activation_derivative(Z[l-1])\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "We propagate `dZ` backwards through the network. At each layer:\n",
    "1. Use `dZ` to compute `dW` and `db`\n",
    "2. Compute `dA_prev` to pass to the previous layer\n",
    "3. Apply activation derivative to get `dZ` for previous layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, y_true, activation='relu'):\n",
    "    \"\"\"\n",
    "    Perform backward pass to compute gradients.\n",
    "    \n",
    "    Args:\n",
    "        parameters: dict, containing W1, b1, W2, b2, ...\n",
    "        cache: dict, containing Z and A from forward pass\n",
    "        y_true: numpy array, shape (n_samples, n_outputs), ground truth\n",
    "        activation: str, activation function used in hidden layers\n",
    "    \n",
    "    Returns:\n",
    "        gradients: dict, containing dW1, db1, dW2, db2, ...\n",
    "    \"\"\"\n",
    "    gradients = {}\n",
    "    num_layers = len(parameters) // 2\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # Get prediction from cache\n",
    "    y_pred = cache[f'A{num_layers}']\n",
    "    \n",
    "    # ========== OUTPUT LAYER ==========\n",
    "    # dL/dZ_out = dL/dy_pred (since output is linear)\n",
    "    # For MSE: dL/dy_pred = (2/n) * (y_pred - y_true)\n",
    "    dZ_current = mean_squared_error_derivative(y_true, y_pred)\n",
    "    \n",
    "    # dW = A_prev.T @ dZ\n",
    "    A_prev = cache[f'A{num_layers - 1}']\n",
    "    gradients[f'dW{num_layers}'] = A_prev.T @ dZ_current\n",
    "    \n",
    "    # db = sum(dZ, axis=0)\n",
    "    gradients[f'db{num_layers}'] = np.sum(dZ_current, axis=0, keepdims=True)\n",
    "    \n",
    "    # ========== HIDDEN LAYERS (going backwards) ==========\n",
    "    # Propagate gradient to previous layer\n",
    "    dA_prev = dZ_current @ parameters[f'W{num_layers}'].T\n",
    "    \n",
    "    for layer_idx in reversed(range(1, num_layers)):\n",
    "        # Get cached values\n",
    "        Z_current = cache[f'Z{layer_idx}']\n",
    "        A_prev = cache[f'A{layer_idx - 1}']\n",
    "        \n",
    "        # Apply activation derivative: dZ = dA * activation'(Z)\n",
    "        if activation == 'relu':\n",
    "            dZ_current = dA_prev * relu_derivative(Z_current)\n",
    "        elif activation == 'sigmoid':\n",
    "            dZ_current = dA_prev * sigmoid_derivative(Z_current)\n",
    "        elif activation == 'tanh':\n",
    "            dZ_current = dA_prev * tanh_derivative(Z_current)\n",
    "        \n",
    "        # Compute gradients for this layer\n",
    "        gradients[f'dW{layer_idx}'] = A_prev.T @ dZ_current\n",
    "        gradients[f'db{layer_idx}'] = np.sum(dZ_current, axis=0, keepdims=True)\n",
    "        \n",
    "        # Propagate to previous layer (if not at first hidden layer)\n",
    "        if layer_idx > 1:\n",
    "            dA_prev = dZ_current @ parameters[f'W{layer_idx}'].T\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "\n",
    "# Test backpropagation\n",
    "y_test = np.array([[0.1], [0.5], [0.9]])\n",
    "grads_test = backward_propagation(test_params, cache_test, y_test)\n",
    "print(\"Backpropagation test:\")\n",
    "print(\"  Gradient shapes:\")\n",
    "for key, value in grads_test.items():\n",
    "    print(f\"    {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 7: Gradient Checking (IMPORTANT FOR INTERVIEWS)\n",
    "\n",
    "### Why Gradient Checking?\n",
    "\n",
    "Backprop is error-prone. Gradient checking verifies correctness by comparing:\n",
    "- **Analytical gradient** (from backprop)\n",
    "- **Numerical gradient** (from finite differences)\n",
    "\n",
    "### The Method\n",
    "\n",
    "For each parameter `theta`:\n",
    "```\n",
    "numerical_grad = (L(theta + epsilon) - L(theta - epsilon)) / (2 * epsilon)\n",
    "```\n",
    "\n",
    "### Relative Error\n",
    "\n",
    "```\n",
    "relative_error = |analytical - numerical| / (|analytical| + |numerical| + epsilon)\n",
    "```\n",
    "\n",
    "- `< 1e-7`: Excellent\n",
    "- `< 1e-5`: Acceptable\n",
    "- `> 1e-3`: Bug in backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(parameters, X, y_true, activation='relu', epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Verify backpropagation correctness using numerical gradients.\n",
    "    \n",
    "    Args:\n",
    "        parameters: dict, network parameters\n",
    "        X: numpy array, input data\n",
    "        y_true: numpy array, ground truth\n",
    "        activation: str, activation function\n",
    "        epsilon: float, small value for finite differences\n",
    "    \n",
    "    Returns:\n",
    "        float, maximum relative error across all parameters\n",
    "    \"\"\"\n",
    "    # Get analytical gradients from backprop\n",
    "    y_pred, cache = forward_propagation(X, parameters, activation)\n",
    "    analytical_grads = backward_propagation(parameters, cache, y_true, activation)\n",
    "    \n",
    "    max_relative_error = 0\n",
    "    \n",
    "    # Check each parameter\n",
    "    for param_name in parameters:\n",
    "        param = parameters[param_name]\n",
    "        grad_name = 'd' + param_name  # dW1, db1, etc.\n",
    "        analytical_grad = analytical_grads[grad_name]\n",
    "        \n",
    "        # Compute numerical gradient for each element\n",
    "        numerical_grad = np.zeros_like(param)\n",
    "        \n",
    "        # Iterate over each element in the parameter\n",
    "        it = np.nditer(param, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            idx = it.multi_index\n",
    "            original_value = param[idx]\n",
    "            \n",
    "            # L(theta + epsilon)\n",
    "            param[idx] = original_value + epsilon\n",
    "            y_plus, _ = forward_propagation(X, parameters, activation)\n",
    "            loss_plus = mean_squared_error(y_true, y_plus)\n",
    "            \n",
    "            # L(theta - epsilon)\n",
    "            param[idx] = original_value - epsilon\n",
    "            y_minus, _ = forward_propagation(X, parameters, activation)\n",
    "            loss_minus = mean_squared_error(y_true, y_minus)\n",
    "            \n",
    "            # Numerical gradient\n",
    "            numerical_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            \n",
    "            # Restore original value\n",
    "            param[idx] = original_value\n",
    "            it.iternext()\n",
    "        \n",
    "        # Compute relative error\n",
    "        numerator = np.abs(analytical_grad - numerical_grad)\n",
    "        denominator = np.abs(analytical_grad) + np.abs(numerical_grad) + epsilon\n",
    "        relative_error = np.max(numerator / denominator)\n",
    "        \n",
    "        max_relative_error = max(max_relative_error, relative_error)\n",
    "        \n",
    "        print(f\"  {param_name}: relative error = {relative_error:.2e}\")\n",
    "    \n",
    "    return max_relative_error\n",
    "\n",
    "\n",
    "# Run gradient check on small network\n",
    "print(\"Gradient Check Results:\")\n",
    "print(\"=\" * 40)\n",
    "small_params = initialize_parameters([1, 3, 3, 1], initialization='he')\n",
    "X_check = np.array([[0.5], [1.0]])\n",
    "y_check = np.array([[0.3], [0.7]])\n",
    "\n",
    "max_error = gradient_check(small_params, X_check, y_check, activation='relu')\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if max_error < 1e-5:\n",
    "    print(f\"PASSED: Max error {max_error:.2e} < 1e-5\")\n",
    "else:\n",
    "    print(f\"WARNING: Max error {max_error:.2e} >= 1e-5 - check backprop!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 8: Parameter Update (Gradient Descent)\n",
    "\n",
    "### Vanilla Gradient Descent\n",
    "\n",
    "**Update rule:** `theta = theta - learning_rate * gradient`\n",
    "\n",
    "### Learning Rate Considerations\n",
    "\n",
    "- **Too large**: Overshoots minimum, may diverge\n",
    "- **Too small**: Slow convergence\n",
    "- **Just right**: Smooth decrease in loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        parameters: dict, current parameters\n",
    "        gradients: dict, gradients from backprop\n",
    "        learning_rate: float, step size\n",
    "    \n",
    "    Returns:\n",
    "        dict, updated parameters\n",
    "    \"\"\"\n",
    "    num_layers = len(parameters) // 2\n",
    "    \n",
    "    for layer_idx in range(1, num_layers + 1):\n",
    "        parameters[f'W{layer_idx}'] -= learning_rate * gradients[f'dW{layer_idx}']\n",
    "        parameters[f'b{layer_idx}'] -= learning_rate * gradients[f'db{layer_idx}']\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 9: Complete Training Pipeline\n",
    "\n",
    "### The Training Loop\n",
    "\n",
    "```\n",
    "for each epoch:\n",
    "    1. Forward pass: compute predictions\n",
    "    2. Compute loss\n",
    "    3. Backward pass: compute gradients\n",
    "    4. Update parameters\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X_train, y_train, layer_dimensions, \n",
    "                  learning_rate=0.01, num_epochs=1000,\n",
    "                  activation='relu', initialization='he',\n",
    "                  print_every=200, seed=42):\n",
    "    \"\"\"\n",
    "    Train neural network using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        X_train: numpy array, shape (n_samples, n_features)\n",
    "        y_train: numpy array, shape (n_samples, n_outputs)\n",
    "        layer_dimensions: list, [n_input, n_hidden1, ..., n_output]\n",
    "        learning_rate: float, gradient descent step size\n",
    "        num_epochs: int, number of training iterations\n",
    "        activation: str, activation function for hidden layers\n",
    "        initialization: str, weight initialization method\n",
    "        print_every: int, print loss every N epochs\n",
    "        seed: int, random seed\n",
    "    \n",
    "    Returns:\n",
    "        parameters: dict, trained parameters\n",
    "        loss_history: list, loss at each epoch\n",
    "    \"\"\"\n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layer_dimensions, initialization, seed)\n",
    "    loss_history = []\n",
    "    \n",
    "    print(f\"Training network: {layer_dimensions}\")\n",
    "    print(f\"Activation: {activation}, LR: {learning_rate}, Epochs: {num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        y_pred, cache = forward_propagation(X_train, parameters, activation)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = mean_squared_error(y_train, y_pred)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Backward propagation\n",
    "        gradients = backward_propagation(parameters, cache, y_train, activation)\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % print_every == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch + 1:5d} | Loss: {loss:.6f}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Final Loss: {loss_history[-1]:.6f}\")\n",
    "    \n",
    "    return parameters, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 10: Experiments on Different Data\n",
    "\n",
    "We test on three types of data to understand network behavior:\n",
    "\n",
    "1. **Sinusoidal**: Non-linear, periodic - tests non-linear function approximation\n",
    "2. **Linear**: Simple relationship - network should easily fit\n",
    "3. **Polynomial**: Non-linear, non-periodic - tests smooth curve fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_datasets(n_samples=200, noise_level=0.2, seed=42):\n",
    "    \"\"\"\n",
    "    Generate three types of regression datasets.\n",
    "    \n",
    "    Returns:\n",
    "        dict with 'sinusoidal', 'linear', 'polynomial' datasets\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X = np.linspace(-2, 2, n_samples).reshape(-1, 1)\n",
    "    noise = noise_level * np.random.randn(n_samples, 1)\n",
    "    \n",
    "    datasets = {\n",
    "        'sinusoidal': {\n",
    "            'X': X,\n",
    "            'y': np.sin(3 * X) + noise,\n",
    "            'description': 'y = sin(3x) + noise'\n",
    "        },\n",
    "        'linear': {\n",
    "            'X': X,\n",
    "            'y': 2.5 * X + 1.0 + noise,\n",
    "            'description': 'y = 2.5x + 1 + noise'\n",
    "        },\n",
    "        'polynomial': {\n",
    "            'X': X,\n",
    "            'y': 1.5 * X**2 + 0.5 * X + 1.0 + noise,\n",
    "            'description': 'y = 1.5x^2 + 0.5x + 1 + noise'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "# Generate datasets\n",
    "datasets = generate_datasets()\n",
    "\n",
    "# Visualize datasets\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets.items()):\n",
    "    axes[idx].scatter(data['X'], data['y'], alpha=0.6, s=15)\n",
    "    axes[idx].set_title(f\"{name.capitalize()}\\n{data['description']}\")\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "NETWORK_ARCHITECTURE = [1, 10, 10, 1]  # Input -> Hidden -> Hidden -> Output\n",
    "LEARNING_RATE = 0.1\n",
    "NUM_EPOCHS = 3000\n",
    "ACTIVATION = 'relu'\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training on {name.upper()} data\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    trained_params, loss_history = train_network(\n",
    "        X_train=data['X'],\n",
    "        y_train=data['y'],\n",
    "        layer_dimensions=NETWORK_ARCHITECTURE,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        activation=ACTIVATION,\n",
    "        print_every=500\n",
    "    )\n",
    "    \n",
    "    results[name] = {\n",
    "        'parameters': trained_params,\n",
    "        'loss_history': loss_history\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "colors = {'sinusoidal': 'blue', 'linear': 'green', 'polynomial': 'red'}\n",
    "\n",
    "for idx, (name, result) in enumerate(results.items()):\n",
    "    axes[idx].plot(result['loss_history'], color=colors[name], linewidth=1)\n",
    "    axes[idx].set_title(f'{name.capitalize()} - Loss Curve')\n",
    "    axes[idx].set_xlabel('Epoch')\n",
    "    axes[idx].set_ylabel('MSE Loss')\n",
    "    axes[idx].set_yscale('log')  # Log scale to see convergence better\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate final loss\n",
    "    final_loss = result['loss_history'][-1]\n",
    "    axes[idx].annotate(f'Final: {final_loss:.4f}', \n",
    "                       xy=(0.95, 0.95), xycoords='axes fraction',\n",
    "                       ha='right', va='top', fontsize=10,\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for idx, (name, data) in enumerate(datasets.items()):\n",
    "    X = data['X']\n",
    "    y_true = data['y']\n",
    "    params = results[name]['parameters']\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred, _ = forward_propagation(X, params, activation=ACTIVATION)\n",
    "    \n",
    "    # Sort for smooth line plot\n",
    "    sort_idx = np.argsort(X[:, 0])\n",
    "    X_sorted = X[sort_idx]\n",
    "    y_pred_sorted = y_pred[sort_idx]\n",
    "    \n",
    "    # Plot\n",
    "    axes[idx].scatter(X, y_true, alpha=0.5, s=15, label='True Data', color='blue')\n",
    "    axes[idx].plot(X_sorted, y_pred_sorted, color='red', linewidth=2, label='Prediction')\n",
    "    axes[idx].set_title(f'{name.capitalize()}')\n",
    "    axes[idx].set_xlabel('X')\n",
    "    axes[idx].set_ylabel('y')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 11: Analysis and Key Insights\n",
    "\n",
    "### Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Convergence Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for name, result in results.items():\n",
    "    loss_history = result['loss_history']\n",
    "    initial_loss = loss_history[0]\n",
    "    final_loss = loss_history[-1]\n",
    "    improvement = (initial_loss - final_loss) / initial_loss * 100\n",
    "    \n",
    "    # Find epoch where loss dropped below threshold\n",
    "    threshold = final_loss * 1.1  # Within 10% of final\n",
    "    convergence_epoch = next((i for i, loss in enumerate(loss_history) if loss < threshold), NUM_EPOCHS)\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Initial Loss: {initial_loss:.6f}\")\n",
    "    print(f\"  Final Loss:   {final_loss:.6f}\")\n",
    "    print(f\"  Improvement:  {improvement:.2f}%\")\n",
    "    print(f\"  Converged at: ~epoch {convergence_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Linear data converges fastest** - The simplest function to approximate\n",
    "\n",
    "2. **Sinusoidal needs more capacity** - High frequency requires network to learn sharp turns\n",
    "\n",
    "3. **Polynomial is intermediate** - Smooth curve, easier than sine but harder than line\n",
    "\n",
    "4. **ReLU creates piecewise linear approximations** - Look at how the red line has \"kinks\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 12: Summary - What to Remember\n",
    "\n",
    "### Core Equations\n",
    "\n",
    "**Forward Pass:**\n",
    "```\n",
    "Z[l] = A[l-1] @ W[l] + b[l]\n",
    "A[l] = activation(Z[l])\n",
    "```\n",
    "\n",
    "**Backward Pass (Chain Rule):**\n",
    "```\n",
    "dZ[L] = dL/dy_pred (output layer)\n",
    "dW[l] = A[l-1].T @ dZ[l]\n",
    "db[l] = sum(dZ[l], axis=0)\n",
    "dA[l-1] = dZ[l] @ W[l].T\n",
    "dZ[l-1] = dA[l-1] * activation'(Z[l-1])\n",
    "```\n",
    "\n",
    "**Update:**\n",
    "```\n",
    "W = W - lr * dW\n",
    "b = b - lr * db\n",
    "```\n",
    "\n",
    "### Interview Points\n",
    "\n",
    "1. **Why cache Z and A?** - Needed for backprop gradient computation\n",
    "\n",
    "2. **Why He init for ReLU?** - ReLU kills half the values, need larger variance\n",
    "\n",
    "3. **Why gradient checking?** - Verify backprop correctness (compare analytical vs numerical)\n",
    "\n",
    "4. **Vanishing gradient?** - Sigmoid/tanh derivatives shrink, ReLU solves with constant gradient\n",
    "\n",
    "5. **Why no activation on output?** - Regression needs unbounded output\n",
    "\n",
    "6. **Matrix dimension rule:** `(n, m) @ (m, p) = (n, p)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Reference Card\n",
    "\n",
    "```\n",
    "ACTIVATION FUNCTIONS\n",
    "--------------------\n",
    "ReLU:     f(z) = max(0, z)           f'(z) = 1 if z > 0 else 0\n",
    "Sigmoid:  f(z) = 1/(1+e^-z)          f'(z) = f(z)(1-f(z))\n",
    "Tanh:     f(z) = (e^z-e^-z)/(e^z+e^-z)   f'(z) = 1-f(z)^2\n",
    "\n",
    "LOSS FUNCTIONS\n",
    "--------------\n",
    "MSE:      L = (1/n) * sum((y-y_hat)^2)    dL/dy_hat = (2/n)(y_hat-y)\n",
    "\n",
    "INITIALIZATION\n",
    "--------------\n",
    "He (ReLU):     W ~ N(0, sqrt(2/n_in))\n",
    "Xavier:        W ~ N(0, sqrt(2/(n_in+n_out)))\n",
    "\n",
    "GRADIENT CHECKING\n",
    "-----------------\n",
    "numerical_grad = (L(theta+eps) - L(theta-eps)) / (2*eps)\n",
    "relative_error = |analytical - numerical| / (|analytical| + |numerical|)\n",
    "Good if < 1e-5\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
