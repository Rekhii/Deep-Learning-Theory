{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QE-IW35O-QTF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BKbOWGrWMSRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, seed=42):\n",
        "        np.random.seed(seed)\n",
        "        self.lr = learning_rate\n",
        "        self.W = [np.random.randn(n_prev, n_next) * np.sqrt(2 / n_prev)\n",
        "                  for n_prev, n_next in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
        "        self.b = [np.zeros((1, n_next)) for n_next in layer_sizes[1:]]\n",
        "\n",
        "    #Activation functions\n",
        "\n",
        "    def _relu(self, x):\n",
        "        return np.maximum(0, x)\n",
        "\n",
        "    def _drelu(self, x):\n",
        "        return (x > 0).astype(float)\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def _dsigmoid(self, x):\n",
        "        s = self._sigmoid(x)\n",
        "        return s * (1 - s)\n",
        "\n",
        "    #Forward & backward passes\n",
        "\n",
        "    def _forward(self, X):\n",
        "\n",
        "        z, a = [], [X]\n",
        "        for idx, (w, b) in enumerate(zip(self.W, self.b)):\n",
        "            z_curr = a[-1] @ w + b  # linear step\n",
        "            if idx == len(self.W) - 1:  # output layer uses sigmoid\n",
        "                a_curr = self._sigmoid(z_curr)\n",
        "            else:                       # hidden layers use ReLU\n",
        "                a_curr = self._relu(z_curr)\n",
        "            z.append(z_curr)\n",
        "            a.append(a_curr)\n",
        "        return z, a\n",
        "\n",
        "    def _backward(self, z, a, y_true):\n",
        "\n",
        "        m = y_true.shape[0]\n",
        "        # Initial gradient (output layer)\n",
        "        dz = (a[-1] - y_true) * self._dsigmoid(z[-1])\n",
        "        for i in reversed(range(len(self.W))):\n",
        "            dw = a[i].T @ dz / m\n",
        "            db = np.sum(dz, axis=0, keepdims=True) / m\n",
        "            # Gradient for next layer (if any)\n",
        "            if i != 0:\n",
        "                dz = (dz @ self.W[i].T) * self._drelu(z[i-1])\n",
        "            # Gradient descent update\n",
        "            self.W[i] -= self.lr * dw\n",
        "            self.b[i] -= self.lr * db\n",
        "\n",
        "\n",
        "    def fit(self, X, y, epochs=1000, verbose=True):\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            z, a = self._forward(X)\n",
        "            self._backward(z, a, y)\n",
        "            if verbose and epoch % (epochs // 10) == 0:\n",
        "                loss = self._loss(y, a[-1])\n",
        "                print(f\"Epoch {epoch}/{epochs} - loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        _, a = self._forward(X)\n",
        "        return (a[-1] > 0.5).astype(int)\n",
        "\n",
        "\n",
        "    def _loss(self, y_true, y_pred):\n",
        "        \"\"\"Binary cross-entropy.\"\"\"\n",
        "        eps = 1e-12  # prevent log(0)\n",
        "        y_pred = np.clip(y_pred, eps, 1 - eps)\n",
        "        m = y_true.shape[0]\n",
        "        return -np.sum(y_true * np.log(y_pred) +\n",
        "                       (1 - y_true) * np.log(1 - y_pred)) / m\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # XOR demo\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn = NeuralNetwork(layer_sizes=[2, 4, 1], learning_rate=0.1)\n",
        "    nn.fit(X, y, epochs=40000, verbose=False)\n",
        "\n",
        "    print(\"Predictions (XOR):\")\n",
        "    for x_i, p_i in zip(X, nn.predict(X)):\n",
        "        print(f\"{x_i} -> {int(p_i)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDFhJ4SKLkbx",
        "outputId": "7d5f226e-0e56-4286-a277-b402d8805f9c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions (XOR):\n",
            "[0 0] -> 0\n",
            "[0 1] -> 1\n",
            "[1 0] -> 1\n",
            "[1 1] -> 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1614287329.py:91: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(f\"{x_i} -> {int(p_i)}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BUAZJ0DlMS8E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}